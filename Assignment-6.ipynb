{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "This assignment requires you to work with Facebook network data, data preprocessing and networkx. Note that this is real data from real people!\n",
    "\n",
    "## Part 1: Preparing data\n",
    "The dataset you will be working with is available here: https://snap.stanford.edu/data/egonets-Facebook.html\n",
    "\n",
    "You're first job is to\n",
    "\n",
    "1. Download the data\n",
    "2. Unpack the data\n",
    "3. Import the data as an undirected graph in networkx\n",
    "\n",
    "This should all be done from your notebook in Python. This is an important step for you to automate data preprocessing.\n",
    "\n",
    "Note: this could take a while, so if you feel adventurous you can use the multiprocessing library to speed things up.\n",
    "\n",
    "Hand-in:\n",
    "- The code for downloading, unpacking and loading the dataset\n",
    "\n",
    "## Part 2: Analyse the data\n",
    "Now, let's take a look at the network you imported.\n",
    "\n",
    "By node degree we mean the number of edges to and from a node. This is different in an undirected network, where in-degree == out-degree, and a directed network where in-degree != out-degree.\n",
    "\n",
    "By graph degree we mean the number of edges in the entire network.\n",
    "\n",
    "Hand-in code that display:\n",
    "- The number of nodes in the network\n",
    "- The number of edges in the network\n",
    "- The average degree in the network\n",
    "- A visualisation of the network inside your notebook\n",
    "\n",
    "## Part 3: Find the most popular people\n",
    "We're naturally interested in who has the most friends, so we want to extract top 10. That is, the 10 most connected people.\n",
    "\n",
    "Hand-in:\n",
    "- Code that extracts and reports the 10 people with the most connections in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "from glob import glob\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout, write_dot\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_url(url):\n",
    "    # downloading data\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    zipfilename = url.split('/')[-1]\n",
    "    filename = zipfilename.replace('.gz','')\n",
    "    \n",
    "    if response.ok:  # status_code == 200:\n",
    "        # opens the file\n",
    "        with open(zipfilename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # extracting filename\n",
    "    with gzip.open(zipfilename, 'rb') as f_in:\n",
    "        with open(filename, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    # finding the file\n",
    "    local_file = glob('*_combined')[0]\n",
    "    return local_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://snap.stanford.edu/data/facebook_combined.txt.gz'\n",
    "filename = get_data_from_url(url)\n",
    "graph = nx.read_edgelist(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(graph):\n",
    "    nx.draw(graph, pos=graphviz_layout(graph), \n",
    "            node_size=10, width=.05, cmap=plt.cm.Blues, \n",
    "            with_labels=False, node_color=range(len(graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 4039\n",
      "Number of edges: 88234\n",
      "Average degree:  43.6910\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(graph))\n",
    "#draw_graph(graph) # virker ikke i mybinder pga. pygraphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took me 381 iterations to converge\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "GLOBAL_PR = np.ones(graph.number_of_nodes())\n",
    "\n",
    "def page_rank(node):\n",
    "    n = graph.number_of_nodes()\n",
    "    damping = 0.85\n",
    "\n",
    "    pr_in = np.array([GLOBAL_PR[int(remote)] for remote, _ in graph.edges(node)])\n",
    "    c = np.array([graph.degree(remote) for remote, _ in graph.edges(node)])\n",
    "    pr_p = ((1 - damping) / n) + (damping * np.sum(pr_in / c))\n",
    "\n",
    "    return pr_p\n",
    "        \n",
    "def compute_page_rank_step(graph):\n",
    "    return np.array([page_rank(n) for n in graph.nodes()])\n",
    "    \n",
    "def compute_page_rank(graph, no_it=100):\n",
    "    global GLOBAL_PR\n",
    "    if no_it == 'converge':\n",
    "        converged = False\n",
    "        it_count = 0\n",
    "        while not converged:\n",
    "            new_pr = compute_page_rank_step(graph)\n",
    "            converged = np.array_equal(GLOBAL_PR, new_pr)\n",
    "            GLOBAL_PR = new_pr \n",
    "            it_count += 1\n",
    "            \n",
    "        print('It took me {} iterations to converge'.format(it_count))\n",
    "    else:\n",
    "        for idx in range(no_it):\n",
    "            GLOBAL_PR = compute_page_rank_step(graph)\n",
    "            \n",
    "compute_page_rank(graph, no_it='converge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0002475860361475619, 14),\n",
      " (0.0002475860361475619, 20),\n",
      " (0.0002475860361475619, 39),\n",
      " (0.0002475860361475619, 57),\n",
      " (0.0002475860361475619, 66),\n",
      " (0.0002475860361475619, 144),\n",
      " (0.0002475860361475619, 227),\n",
      " (0.0002475860361475619, 258),\n",
      " (0.0002475860361475617, 21),\n",
      " (0.0002475860361475617, 48)]\n"
     ]
    }
   ],
   "source": [
    "sorted_GLOBAL_PR = sorted(zip(GLOBAL_PR,range(graph.number_of_nodes())),key= lambda x: x[0],reverse=True)\n",
    "pp(sorted_GLOBAL_PR[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
